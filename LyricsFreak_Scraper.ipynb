{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LyricsFreak Scraper",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "19Ze0juV2sJllfxA_66siBJPSEftMWqNT",
      "authorship_tag": "ABX9TyPLgVXVdpFS9UhlUNHuhe/R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeFane/LyricsFreak-Scraper/blob/main/LyricsFreak_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ6vxl3BNaxu"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3HF-RS6NXZt"
      },
      "source": [
        "I'm working with some friends on a lyric analyzer (trying not to be too specific). It will be pretty ambitious with a lot of web scraping and data analysis, so we're building it piece by piece. I started with the web scraping side of things because I have experience from building the MDining Scraper.  The app on this page will be a small part of our project.\r\n",
        "\r\n",
        "The idea is to start with an artist name and count the frequency of all words in all their songs. Breaking it down further, we pick a site that lists artists and stores lyrics, go to an artist's page, get links to all their songs' lyrics, follow those links to each song's page, scrape the lyrics off that page, combine all songs' lyrics into a list or string, and turn that into a frequency dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKfADYVIiKm"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A9YTPyCIgMC"
      },
      "source": [
        "## Required Libraries "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deADV-d_GwpF"
      },
      "source": [
        "Requests for getting a link's content, asyncio for requesting multiple links concurrently, nest_asyncio to prevent async errors, lxml for parsing that content to an HTML object, re for finding words in a string through regex, collections.Counter for quickly making frequency dictionaries, datetime for timing performance, json for dumping data to files, and pandas for displaying data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl1FuJe4GvuM"
      },
      "source": [
        "import requests\r\n",
        "import asyncio\r\n",
        "import nest_asyncio\r\n",
        "from lxml.html import fromstring, tostring\r\n",
        "import re\r\n",
        "from collections import Counter\r\n",
        "from datetime import datetime as dt\r\n",
        "import json\r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7Ic2gWyNk80"
      },
      "source": [
        "## Links for Artist and Songs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOZGPRC1INl4"
      },
      "source": [
        "Takes a name and creates the LyricFreak link. For the example of Hans Zimmer, the link is https://www.lyricsfreak.com/h/hans+zimmer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOOu_HIOIM72"
      },
      "source": [
        "name = 'hans zimmer'\r\n",
        "start = dt.now()\r\n",
        "\r\n",
        "single = name.lower().replace(' ', '+')\r\n",
        "site = 'https://www.lyricsfreak.com'\r\n",
        "link = f\"{site}/{single[0]}/{single}/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGzI6Z5IpGB"
      },
      "source": [
        "Gets all song links for an artist. Looking at the XPath query, we first look find divs with class=\"lf-list js-sort-table\". We do this because there are multiple divs on the page, and only this one contains songs with the artist as primary credit. Another div contains additional songs, but those are just features and we don't want to get those.\r\n",
        "\r\n",
        "Inside the div we want, we get the href tags for each a component, because each is a song. We store all the song links in 'songs'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL9tRmUnIl4I"
      },
      "source": [
        "r = requests.get(link)\r\n",
        "tree = fromstring(r.content)\r\n",
        "\r\n",
        "path = '//div[@class=\"lf-list js-sort-table\"]//a[@class=\"lf-link lf-link--primary\"]/@href'\r\n",
        "songs = tree.xpath(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-tcRSVANxLS"
      },
      "source": [
        "## Asynchronous Requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkYkqTW0Jl4N"
      },
      "source": [
        "I began with a straightforward for loop to request.get all song pages. However, doing things sequentially took way too long, maybe half a second per page. I knew I had to start learning about asynchronous programming.\r\n",
        "\r\n",
        "The first line I have in this code because it prevents errors. I saw an error, searched StackOverflow, and copied that line of code, and the error stopped occurring. I don't get it but I don't question it.\r\n",
        "\r\n",
        "I then create future tasks, each of which requests a song page's content. I gather all future responses, then dump all of them to 'resps.txt'. I dump 'resps' because I can't access variables inside an async function. Any inits or assignments seem to be completely local.  \r\n",
        "\r\n",
        "I could've used grequests or aiohttp for this, but I couldn't figure out how to work them. With just asyncio and requests, though, I nicely don't have to pip install any additional libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1vdEa9iJio-"
      },
      "source": [
        "nest_asyncio.apply()\r\n",
        "async def main(loop):\r\n",
        "    futures = [loop.run_in_executor(None, requests.get, site + song) for song in songs]\r\n",
        "    resps = await asyncio.gather(*futures)\r\n",
        "    with open('/content/resps.txt', 'w') as f:\r\n",
        "        json.dump(\r\n",
        "            [resp.content.decode('utf8') for resp in resps], \r\n",
        "            f, indent=4\r\n",
        "        )\r\n",
        "\r\n",
        "loop = asyncio.get_event_loop()\r\n",
        "loop.run_until_complete(main(loop))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSH5EUXuO4ia"
      },
      "source": [
        "## Getting All Lyrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQie_7piO_6h"
      },
      "source": [
        "I retrieve all pages' contents with json.load and convert them into HTML objects with fromstring. I then get every line of lyrics for every song and combine them into a flat list of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TnjX9mIPcrC"
      },
      "source": [
        "with open('/content/resps.txt', 'r') as f:\r\n",
        "    trees = [fromstring(content) for content in json.load(f)]\r\n",
        "\r\n",
        "path = '//div[@class=\"lyrictxt js-lyrics js-share-text-content\"]/text()'\r\n",
        "lines = [line for tree in trees for line in tree.xpath(path)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suTLeOXuQjXM"
      },
      "source": [
        "Then, I join all lines into a single string and use a regex to find all words, including those with apostrophes and dashes. I turn that list of words into a frequency dictionary with Counter and display it in a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmmZVraTFmJh"
      },
      "source": [
        "whole = ' '.join(lines)\r\n",
        "words = re.findall(\"[\\w'-]+\", whole.lower())\r\n",
        "c = Counter(words)\r\n",
        "\r\n",
        "print('Elapsed:', dt.now() - start)\r\n",
        "print(len(c), 'different words')\r\n",
        "if len(c) == 0:\r\n",
        "    print('Probably no artist with that name')\r\n",
        "print()\r\n",
        "\r\n",
        "%load_ext google.colab.data_table\r\n",
        "pd.DataFrame({\r\n",
        "    'Word': c.keys(),\r\n",
        "    'Count': c.values(),\r\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isbvhoxkRAwU"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQhn_vd9RCJP"
      },
      "source": [
        "I'm delighted to work with web scraping again, and the final project will be quite cool once you see it.\r\n",
        "\r\n",
        "---\r\n",
        "Thank you for reading!\r\n",
        "\r\n",
        "George Fane\r\n",
        "\r\n",
        "With Alex Beloiu and Yongwei Che"
      ]
    }
  ]
}